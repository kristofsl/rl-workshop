{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "btOKapbhZlFX"
   },
   "outputs": [],
   "source": [
    "!pip install optuna==3.2.0\n",
    "!pip install gymnasium==0.28.1\n",
    "!pip install renderlab\n",
    "!pip install keras==2.13.1\n",
    "!pip install tensorflow==2.13.0rc2\n",
    "!pip install tensorflow-estimator==2.13.0\n",
    "!pip install tensorflow-macos==2.13.0rc2\n",
    "!pip install tensorboard==2.13.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FhHxtnSXZpaO"
   },
   "outputs": [],
   "source": [
    "from typing import Tuple, List, Callable, Union, Optional, Dict\n",
    "import numpy as np\n",
    "import os\n",
    "import pathlib\n",
    "import imageio\n",
    "from pathlib import Path\n",
    "import pandas as pdbb\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "from pathlib import Path\n",
    "from gym.wrappers import RecordVideo\n",
    "import datetime\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from collections import deque\n",
    "import time\n",
    "import random\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "import os\n",
    "import optuna\n",
    "from optuna.importance import get_param_importances\n",
    "import uuid\n",
    "import renderlab as rl\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YSYGVxOayXvx"
   },
   "outputs": [],
   "source": [
    "def evaluate(\n",
    "    agent,\n",
    "    env,\n",
    "    n_episodes: int,\n",
    "    observation_space_size: int\n",
    ") -> Tuple[List, List]:\n",
    "\n",
    "    # keep track of the reward and steps per episode\n",
    "    reward_per_episode = []\n",
    "    steps_per_episode = []\n",
    "\n",
    "    for i in range(0, n_episodes):\n",
    "\n",
    "        # reset and initial state from environment needs reshape to np array (tuple instead of array with dimensions 1,4)\n",
    "        state = np.reshape(env.reset()[0], [1, observation_space_size])\n",
    "\n",
    "        # initialize the cumulative steps and the rewards for this episode\n",
    "        rewards = 0\n",
    "        steps = 0\n",
    "\n",
    "        # done = environment ended badly / environment ended after 500 episodes\n",
    "        done = False\n",
    "        truncated = False\n",
    "\n",
    "        while not (done or truncated):\n",
    "\n",
    "            # determine action based on the state\n",
    "            action = agent.act(state)\n",
    "\n",
    "            # take the action and observe the reward, new state, etc\n",
    "            new_state, reward, done, truncated, info = env.step(action)\n",
    "\n",
    "            # enchange the reward signal\n",
    "            reward = enhance_reward_signal(new_state, reward, done, truncated, steps)\n",
    "\n",
    "            # add the data from this step to the episode data\n",
    "            rewards += reward\n",
    "            steps += 1\n",
    "\n",
    "            # reshape the new state\n",
    "            state = np.reshape(new_state, [1, observation_space_size])\n",
    "\n",
    "        reward_per_episode.append(rewards)\n",
    "        steps_per_episode.append(steps)\n",
    "\n",
    "    return reward_per_episode, steps_per_episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YQPyQG-5yncg"
   },
   "outputs": [],
   "source": [
    "# random agent\n",
    "class RandomAgent:\n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "\n",
    "    def act(self, state: np.array) -> int:\n",
    "        return self.env.action_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mTzg3HsVzoGF"
   },
   "outputs": [],
   "source": [
    "def enhance_reward_signal(next_state, reward, done, truncated, steps):\n",
    "    if done:\n",
    "        reward = -100\n",
    "    elif truncated:\n",
    "        reward = 100\n",
    "    elif next_state[0] > 0.2:\n",
    "        reward = -5\n",
    "    elif next_state[0] < -0.2:\n",
    "        reward = -5\n",
    "\n",
    "    return reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W4TRoRYKytET"
   },
   "outputs": [],
   "source": [
    "def record_random_agent():\n",
    "    # trying the random agent first\n",
    "    env = gym.make('CartPole-v1',render_mode = \"rgb_array\")\n",
    "    env = rl.RenderFrame(env, \"./output\")\n",
    "\n",
    "    observation_space_size = env.observation_space.shape[0]\n",
    "\n",
    "    # create a random agent\n",
    "    random_agent = RandomAgent(env)\n",
    "\n",
    "    # evaluate the random agent\n",
    "    rewards, steps = evaluate(random_agent, env, 5, observation_space_size)\n",
    "\n",
    "    env.play()\n",
    "\n",
    "    # evaluation results\n",
    "    median_steps = np.median(steps)\n",
    "    mean_steps = np.mean(steps)\n",
    "    std_steps = np.std(steps)\n",
    "\n",
    "    print(f'median steps = {median_steps}')\n",
    "    print(f'std steps    = {std_steps}')\n",
    "    print(f'mean steps   = {mean_steps}')\n",
    "\n",
    "    return rewards, steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l0TGQmNuplDJ"
   },
   "outputs": [],
   "source": [
    "def record_trained_agent(agent, env):\n",
    "    env = rl.RenderFrame(env, \"./output\")\n",
    "\n",
    "    observation_space_size = env.observation_space.shape[0]\n",
    "\n",
    "    # evaluate the random agent\n",
    "    rewards, steps = evaluate(agent, env, 1, observation_space_size)\n",
    "\n",
    "    env.play()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "huM3ocq5yZME"
   },
   "outputs": [],
   "source": [
    "# record a random agent\n",
    "rewards, steps = record_random_agent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M6_conWu0S0i"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (10, 4))\n",
    "ax.set_title(\"Steps\")\n",
    "pd.Series(steps).plot(kind='hist', bins=100)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qqtZGB_dU2FN"
   },
   "outputs": [],
   "source": [
    "def sample_hyper_parameters(trial: optuna.trial.Trial) -> Dict:\n",
    "\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [16, 32, 64, 128])\n",
    "    memory_size = trial.suggest_int(\"memory_size\", 500, 10000)\n",
    "    gamma = trial.suggest_float('gamma', 0.9, 0.99)\n",
    "    exploration_decay = trial.suggest_categorical('exploration_decay', [0.9, 0.95, 0.98, 0.99])\n",
    "    layer_size = trial.suggest_categorical('layer_size',[16,32,64,128,256, 512])\n",
    "    exploration_min = trial.suggest_float('exploration_min',0.001, 0.2)\n",
    "    episodes = trial.suggest_int(\"episodes\", 100, 200)\n",
    "    learning_rate = trial.suggest_categorical('learning_rate',[0.001,0.0001])\n",
    "    extra_intermediate_layers = trial.suggest_categorical('extra_layers',[0,1])\n",
    "\n",
    "    return {\n",
    "        'batch_size': batch_size,\n",
    "        'memory_size': memory_size,\n",
    "        'gamma': gamma,\n",
    "        'exploration_decay' : exploration_decay,\n",
    "        'layer_size' : layer_size,\n",
    "        'exploration_min' : exploration_min,\n",
    "        'episodes' : episodes,\n",
    "        'learning_rate' : learning_rate,\n",
    "        'extra_intermediate_layers' : extra_intermediate_layers\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "klvvKg5OUJnk"
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "from keras.callbacks import Callback\n",
    "\n",
    "class MemoryClear(Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fIWyHHHLU7is"
   },
   "outputs": [],
   "source": [
    "memory_clear_callback = MemoryClear()\n",
    "\n",
    "class DeepQAgent:\n",
    "\n",
    "    def __init__(self, env, memory_size, gamma, exploration_decay, layer_size, batch_size, exploration_min, learning_rate, extra_intermediate_layers = 0, exploration_rate = 1.0):\n",
    "        self.__env = env\n",
    "        self.__state_size = 4                                           # we have an 4 numbers that represent our size and this will be the input for our neural net\n",
    "        self.__action_size = 2                                          # we have 2 possible actions (push the card to the left or to the right)\n",
    "        self.__memory = deque(maxlen=memory_size)                       # memory for storing our experiences\n",
    "        self.__layer_size_1 = layer_size                                # nn layer 1 width\n",
    "        self.__layer_size_2 = layer_size                                # nn layer 2 width\n",
    "        self.__extra_intermediate_layers = extra_intermediate_layers    # extra intermediate layers?\n",
    "        self.__learning_rate = learning_rate                            # learning rate\n",
    "        self.__model = self._build_model()                              # our model\n",
    "        self.__gamma = gamma                                            # discount rate\n",
    "        self.__exploration_rate = exploration_rate                      # exploration rate\n",
    "        self.__exploration_min = exploration_min                        # min value for exploration\n",
    "        self.__exploration_decay = exploration_decay                    # decay in the exploration rate (moving from exploration to exploitation)\n",
    "        self.__step_counter = 0                                         # keep track of the steps and update the target model after target_model_delay steps\n",
    "        self.__sample_batch_size = batch_size                           # how much samples to we take for the learning step\n",
    "        self.__uuid = str(uuid.uuid4())                                 # unique uid for the model\n",
    "\n",
    "\n",
    "    def add_to_memory(self, state, action, reward, next_state, done):\n",
    "        self.__memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def disable_exploration(self):\n",
    "        self.__exploration_rate = 0\n",
    "        self.__exploration_min = 0\n",
    "\n",
    "    def get_agent_uuid(self):\n",
    "        return self.__uuid\n",
    "\n",
    "    def save_model(self):\n",
    "        self.__model.save(f'model-{self.__uuid}.keras')\n",
    "\n",
    "    def replay(self):\n",
    "\n",
    "        if len(self.__memory) <= self.__sample_batch_size:\n",
    "            return\n",
    "        else:\n",
    "            mini_batch = random.sample(self.__memory, self.__sample_batch_size)\n",
    "\n",
    "            current_state = np.zeros((self.__sample_batch_size, self.__state_size))\n",
    "            next_state = np.zeros((self.__sample_batch_size, self.__state_size))\n",
    "            target_q_values = np.zeros((self.__sample_batch_size, self.__state_size))\n",
    "\n",
    "            action = np.zeros(self.__sample_batch_size, dtype=int)\n",
    "            reward = np.zeros(self.__sample_batch_size)\n",
    "            done = np.zeros(self.__sample_batch_size,dtype=bool)\n",
    "\n",
    "            for i in range(self.__sample_batch_size):\n",
    "                current_state[i] = mini_batch[i][0]   # state\n",
    "                action[i] = mini_batch[i][1]          # action\n",
    "                reward[i] = mini_batch[i][2]          # reward\n",
    "                next_state[i] = mini_batch[i][3]      # next_state\n",
    "                done[i] = mini_batch[i][4]            # done\n",
    "\n",
    "            target = self.__model.predict(current_state,verbose=0)\n",
    "            Qvalue_ns = self.__model.predict(next_state,verbose=0)\n",
    "\n",
    "            for i in range(self.__sample_batch_size):\n",
    "                if done[i]:\n",
    "                    target[i][action[i]] = reward[i]\n",
    "                else:\n",
    "                    target[i][action[i]] = reward[i] + self.__gamma * (np.amax(Qvalue_ns[i]))\n",
    "\n",
    "\n",
    "            self.__model.fit(current_state, target, batch_size=self.__sample_batch_size, epochs=1, verbose=0, callbacks=[memory_clear_callback])\n",
    "\n",
    "            if self.__exploration_rate > self.__exploration_min:\n",
    "                self.__exploration_rate *= self.__exploration_decay\n",
    "\n",
    "\n",
    "    def act(self, state: np.array) -> int:\n",
    "        if np.random.rand() < self.__exploration_rate:\n",
    "            random_action = self.__env.action_space.sample()\n",
    "            return random_action\n",
    "        else:\n",
    "            actions = self.__model.predict(state, verbose=0)             # get a model prediction\n",
    "            best_action = np.argmax(actions[0])                          # get the output index with the highest value\n",
    "            return best_action\n",
    "\n",
    "    def _build_model(self):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(self.__layer_size_1, input_dim=self.__state_size, activation='relu'))\n",
    "        model.add(Dense(self.__layer_size_2, activation='relu'))\n",
    "        if (self.__extra_intermediate_layers > 0):\n",
    "            for i in range(self.__extra_intermediate_layers + 1):\n",
    "                model.add(Dense(self.__layer_size_2, activation='relu'))\n",
    "        model.add(Dense(self.__action_size, activation='linear'))\n",
    "        model.compile(loss='mse', optimizer=tf.keras.optimizers.legacy.Adam(learning_rate=self.__learning_rate))\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N3PP8R26WHup"
   },
   "outputs": [],
   "source": [
    "# train loop for the deep q agent\n",
    "def optimize_train(agent, env, observation_space_size, n_episodes):\n",
    "    # print('starting traing of agent ...')\n",
    "\n",
    "    for i in range(0, n_episodes):\n",
    "        # reset and reshape the state\n",
    "        state = np.reshape(env.reset()[0], [1, observation_space_size])\n",
    "\n",
    "        print(f'new train episode {i} on : {datetime.datetime.now()}')\n",
    "\n",
    "        # initialize done (fail) and truncated (success)\n",
    "        done = False\n",
    "        truncated = False\n",
    "\n",
    "        steps = 0\n",
    "\n",
    "        # execute the garbase collector\n",
    "        gc.collect()\n",
    "\n",
    "        # clear all the memory\n",
    "        keras.backend.clear_session()\n",
    "\n",
    "        while not (done or truncated):\n",
    "\n",
    "            steps += 1\n",
    "\n",
    "            # take action based on the state\n",
    "            action = agent.act(state)\n",
    "\n",
    "            # observe the signal from the environment after the action\n",
    "            next_state, reward, done, truncated, info = env.step(action)\n",
    "\n",
    "            # enhance the signal and merge the truncated and done signals\n",
    "            reward = enhance_reward_signal(next_state, reward, done, truncated, steps)\n",
    "\n",
    "            if done:\n",
    "                print(f'ended training episode with mistake and reward {reward} and step count {steps}')\n",
    "            elif truncated:\n",
    "                print(f'ended training episode with success and reward {reward} and step count {steps}')\n",
    "\n",
    "            next_state = np.reshape(next_state, [1, observation_space_size])\n",
    "\n",
    "            # add the observation to the memory\n",
    "            agent.add_to_memory(state, action, reward, next_state, done)\n",
    "\n",
    "            # replay observations from the memory and learn if needed\n",
    "            agent.replay()\n",
    "\n",
    "            # replace the current state\n",
    "            state = next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gfXpyMZLVK0O"
   },
   "outputs": [],
   "source": [
    "def objective(trial: optuna.trial.Trial) -> float:\n",
    "\n",
    "    # create an environment\n",
    "    env = gym.make('CartPole-v1',render_mode = \"rgb_array\")\n",
    "\n",
    "    # fetch properties from environment\n",
    "    observation_space_size = env.observation_space.shape[0]\n",
    "    action_space_size = env.action_space.n\n",
    "\n",
    "    print(f'observation space size : {observation_space_size}')\n",
    "    print(f'action space size      : {action_space_size}')\n",
    "\n",
    "    # get the hyper parameter config\n",
    "    args = sample_hyper_parameters(trial)\n",
    "\n",
    "    print(f'optuna run with params started with params : {trial.params}')\n",
    "\n",
    "    # create the deep agent\n",
    "    deep_agent = DeepQAgent(\n",
    "        env,\n",
    "        memory_size = args['memory_size'],\n",
    "        gamma = args['gamma'],\n",
    "        exploration_decay = args['exploration_decay'],\n",
    "        layer_size = args['layer_size'],\n",
    "        batch_size = args['batch_size'],\n",
    "        exploration_min = args['exploration_min'],\n",
    "        learning_rate = args['learning_rate'],\n",
    "        extra_intermediate_layers = args['extra_intermediate_layers']\n",
    "    )\n",
    "\n",
    "    print('train loop started')\n",
    "\n",
    "    # reset all\n",
    "    env.reset()\n",
    "\n",
    "    # train loop\n",
    "    optimize_train(deep_agent,env, observation_space_size,n_episodes=args['episodes'])\n",
    "\n",
    "    # disable exploitation and only rely on the model for decisions\n",
    "    deep_agent.disable_exploration()\n",
    "\n",
    "    print('evaluation started')\n",
    "    rewards, steps = evaluate(deep_agent, env, 50, observation_space_size)\n",
    "\n",
    "    # evaluation results\n",
    "    median_steps = np.median(steps)\n",
    "    mean_steps = np.mean(steps)\n",
    "    std_steps = np.std(steps)\n",
    "\n",
    "    median_reward = np.median(rewards)\n",
    "    mean_reward = np.mean(rewards)\n",
    "    std_reward= np.std(rewards)\n",
    "\n",
    "\n",
    "    print(f'median reward = {median_reward}')\n",
    "    print(f'std reward    = {std_reward}')\n",
    "    print(f'mean reward   = {mean_reward}')\n",
    "\n",
    "    print(f'median steps = {median_steps}')\n",
    "    print(f'std steps    = {std_steps}')\n",
    "    print(f'mean steps   = {mean_steps}')\n",
    "\n",
    "    # record the agent\n",
    "    record_trained_agent(deep_agent,env)\n",
    "\n",
    "    # clear all the memory\n",
    "    keras.backend.clear_session()\n",
    "\n",
    "    # execute the garbase collector\n",
    "    gc.collect()\n",
    "\n",
    "    return median_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s8doAIy3VTKM"
   },
   "outputs": [],
   "source": [
    "# create an optimized model\n",
    "study_name = \"rl-study\"\n",
    "storage_name = \"sqlite:///{}.db\".format(study_name)\n",
    "study = optuna.create_study(study_name=study_name, storage=storage_name,direction='maximize',load_if_exists=True)\n",
    "\n",
    "study.optimize(objective, n_trials=3, gc_after_trial=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IooChA6qgXih"
   },
   "outputs": [],
   "source": [
    "# print param importances\n",
    "print(get_param_importances(study))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-pysxFBHgSNJ"
   },
   "outputs": [],
   "source": [
    "df = study.trials_dataframe()\n",
    "df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fKjJ_3D6iITv"
   },
   "outputs": [],
   "source": [
    "study.best_params"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
