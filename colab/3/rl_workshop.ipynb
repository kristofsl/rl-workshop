{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "btOKapbhZlFX"
      },
      "outputs": [],
      "source": [
        "!pip install optuna==3.2.0\n",
        "!pip install gymnasium==0.28.1\n",
        "!pip install renderlab\n",
        "!pip install keras==2.13.1\n",
        "!pip install tensorflow==2.13.0rc2\n",
        "!pip install tensorflow-estimator==2.13.0\n",
        "!pip install tensorflow-macos==2.13.0rc2\n",
        "!pip install tensorboard==2.13.0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FhHxtnSXZpaO"
      },
      "outputs": [],
      "source": [
        "from typing import Tuple, List, Callable, Union, Optional, Dict\n",
        "import numpy as np\n",
        "import os\n",
        "import pathlib\n",
        "import imageio\n",
        "from pathlib import Path\n",
        "import pandas as pdbb\n",
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from matplotlib import pyplot as plt\n",
        "from pathlib import Path\n",
        "from gym.wrappers import RecordVideo\n",
        "import datetime\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "from collections import deque\n",
        "import time\n",
        "import random\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.optimizers import Adam\n",
        "import os\n",
        "import optuna\n",
        "from optuna.importance import get_param_importances\n",
        "import uuid\n",
        "import renderlab as rl\n",
        "import gc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YSYGVxOayXvx"
      },
      "outputs": [],
      "source": [
        "def evaluate(\n",
        "    agent,\n",
        "    env,\n",
        "    n_episodes: int,\n",
        "    observation_space_size: int\n",
        ") -> Tuple[List, List]:\n",
        "\n",
        "    # keep track of the reward and steps per episode\n",
        "    reward_per_episode = []\n",
        "    steps_per_episode = []\n",
        "\n",
        "    for i in range(0, n_episodes):\n",
        "\n",
        "        # reset and initial state from environment needs reshape to np array (tuple instead of array with dimensions 1,4)\n",
        "        state = np.reshape(env.reset()[0], [1, observation_space_size])\n",
        "\n",
        "        # initialize the cumulative steps and the rewards for this episode\n",
        "        rewards = 0\n",
        "        steps = 0\n",
        "\n",
        "        # done = environment ended badly / environment ended after 500 episodes\n",
        "        done = False\n",
        "        truncated = False\n",
        "\n",
        "        while not (done or truncated):\n",
        "\n",
        "            # determine action based on the state\n",
        "            action = agent.act(state)\n",
        "\n",
        "            # take the action and observe the reward, new state, etc\n",
        "            new_state, reward, done, truncated, info = env.step(action)\n",
        "\n",
        "            # enchange the reward signal\n",
        "            reward = enhance_reward_signal(new_state, reward, done, truncated, steps)\n",
        "\n",
        "            # add the data from this step to the episode data\n",
        "            rewards += reward\n",
        "            steps += 1\n",
        "\n",
        "            # reshape the new state\n",
        "            state = np.reshape(new_state, [1, observation_space_size])\n",
        "\n",
        "        reward_per_episode.append(rewards)\n",
        "        steps_per_episode.append(steps)\n",
        "\n",
        "    return reward_per_episode, steps_per_episode"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YQPyQG-5yncg"
      },
      "outputs": [],
      "source": [
        "# random agent\n",
        "class RandomAgent:\n",
        "    def __init__(self, env):\n",
        "        self.env = env\n",
        "\n",
        "    def act(self, state: np.array) -> int:\n",
        "        return self.env.action_space.sample()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mTzg3HsVzoGF"
      },
      "outputs": [],
      "source": [
        "def enhance_reward_signal(next_state, reward, done, truncated, steps):\n",
        "    if done:\n",
        "        reward = -100\n",
        "    elif truncated:\n",
        "        reward = 100\n",
        "    elif next_state[0] > 0.2:\n",
        "        reward = -5\n",
        "    elif next_state[0] < -0.2:\n",
        "        reward = -5\n",
        "\n",
        "    return reward"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W4TRoRYKytET"
      },
      "outputs": [],
      "source": [
        "def record_random_agent():\n",
        "    # trying the random agent first\n",
        "    env = gym.make('CartPole-v1',render_mode = \"rgb_array\")\n",
        "    env = rl.RenderFrame(env, \"./output\")\n",
        "\n",
        "    observation_space_size = env.observation_space.shape[0]\n",
        "\n",
        "    # create a random agent\n",
        "    random_agent = RandomAgent(env)\n",
        "\n",
        "    # evaluate the random agent\n",
        "    rewards, steps = evaluate(random_agent, env, 5, observation_space_size)\n",
        "\n",
        "    env.play()\n",
        "\n",
        "    # evaluation results\n",
        "    median_steps = np.median(steps)\n",
        "    mean_steps = np.mean(steps)\n",
        "    std_steps = np.std(steps)\n",
        "\n",
        "    print(f'median steps = {median_steps}')\n",
        "    print(f'std steps    = {std_steps}')\n",
        "    print(f'mean steps   = {mean_steps}')\n",
        "\n",
        "    return rewards, steps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l0TGQmNuplDJ"
      },
      "outputs": [],
      "source": [
        "def record_trained_agent(agent, env):\n",
        "    env = rl.RenderFrame(env, \"./output\")\n",
        "\n",
        "    observation_space_size = env.observation_space.shape[0]\n",
        "\n",
        "    # evaluate the random agent\n",
        "    rewards, steps = evaluate(agent, env, 1, observation_space_size)\n",
        "\n",
        "    env.play()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "huM3ocq5yZME"
      },
      "outputs": [],
      "source": [
        "# record a random agent\n",
        "rewards, steps = record_random_agent()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M6_conWu0S0i"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "fig, ax = plt.subplots(figsize = (10, 4))\n",
        "ax.set_title(\"Steps\")\n",
        "pd.Series(steps).plot(kind='hist', bins=100)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qqtZGB_dU2FN"
      },
      "outputs": [],
      "source": [
        "def sample_hyper_parameters(trial: optuna.trial.Trial) -> Dict:\n",
        "\n",
        "    batch_size = trial.suggest_categorical(\"batch_size\", [16, 32, 64, 128])\n",
        "    memory_size = trial.suggest_int(\"memory_size\", 500, 10000)\n",
        "    gamma = trial.suggest_float('gamma', 0.9, 0.99)\n",
        "    exploration_decay = trial.suggest_categorical('exploration_decay', [0.9, 0.95, 0.98, 0.99])\n",
        "    layer_size = trial.suggest_categorical('layer_size',[16,32,64,128,256, 512])\n",
        "    exploration_min = trial.suggest_float('exploration_min',0.001, 0.2)\n",
        "    episodes = trial.suggest_int(\"episodes\", 100, 200)\n",
        "    learning_rate = trial.suggest_categorical('learning_rate',[0.001,0.0001])\n",
        "    extra_intermediate_layers = trial.suggest_categorical('extra_layers',[0,1])\n",
        "    update_model_steps = trial.suggest_categorical('update_model_steps',[10, 20, 30])\n",
        "\n",
        "    return {\n",
        "        'batch_size': batch_size,\n",
        "        'memory_size': memory_size,\n",
        "        'gamma': gamma,\n",
        "        'exploration_decay' : exploration_decay,\n",
        "        'layer_size' : layer_size,\n",
        "        'exploration_min' : exploration_min,\n",
        "        'episodes' : episodes,\n",
        "        'learning_rate' : learning_rate,\n",
        "        'extra_intermediate_layers' : extra_intermediate_layers,\n",
        "         'update_model_steps' : update_model_steps\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "klvvKg5OUJnk"
      },
      "outputs": [],
      "source": [
        "import gc\n",
        "from keras.callbacks import Callback\n",
        "\n",
        "class MemoryClear(Callback):\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fIWyHHHLU7is"
      },
      "outputs": [],
      "source": [
        "memory_clear_callback = MemoryClear()\n",
        "\n",
        "class DeepQAgent:\n",
        "\n",
        "    def __init__(self, env, memory_size, gamma, exploration_decay, layer_size, batch_size, exploration_min, learning_rate, update_model_steps = 20, extra_intermediate_layers = 0, exploration_rate = 1.0):\n",
        "        self.__env = env\n",
        "        self.__state_size = 4                                           # we have an 4 numbers that represent our size and this will be the input for our neural net\n",
        "        self.__action_size = 2                                          # we have 2 possible actions (push the card to the left or to the right)\n",
        "        self.__memory_size = memory_size\n",
        "        self.__memory = deque(maxlen=memory_size)                       # memory for storing our experiences\n",
        "        self.__layer_size_1 = layer_size                                # nn layer 1 width\n",
        "        self.__layer_size_2 = layer_size                                # nn layer 2 width\n",
        "        self.__extra_intermediate_layers = extra_intermediate_layers    # extra intermediate layers?\n",
        "        self.__learning_rate = learning_rate                            # learning rate\n",
        "        self.__model = self._build_model()                              # our model\n",
        "        self.__target_model = self._build_model()                       # our target model\n",
        "        self.__gamma = gamma                                            # discount rate\n",
        "        self.__exploration_rate = exploration_rate                      # exploration rate\n",
        "        self.__exploration_min = exploration_min                        # min value for exploration\n",
        "        self.__exploration_decay = exploration_decay                    # decay in the exploration rate (moving from exploration to exploitation)\n",
        "        self.__sample_batch_size = batch_size                           # how much samples to we take for the learning step\n",
        "        self.__uuid = str(uuid.uuid4())                                 # unique uid for the model\n",
        "        self.__update_model_steps = update_model_steps                  # interval for synchronisation models\n",
        "        self.__internal_learn_counter = 0                               # learn counter that we use for synchronisation between model and target model\n",
        "\n",
        "    def add_to_memory(self, state, action, reward, next_state, done):\n",
        "        # add data to the memory\n",
        "        self.__memory.append((state, action, reward, next_state, done))\n",
        "\n",
        "    def disable_exploration(self):\n",
        "        # disable exploration and go for exploitation\n",
        "        self.__exploration_rate = 0\n",
        "        self.__exploration_min = 0\n",
        "\n",
        "    def reset_episode(self):\n",
        "        self.__internal_step_counter = 0\n",
        "\n",
        "    def get_agent_uuid(self):\n",
        "        # return unique uuid for each created agent\n",
        "        return self.__uuid\n",
        "\n",
        "    def _synch_models(self):\n",
        "        # update the target model with the weights from our model (which is updated every step)\n",
        "        self.__target_model.set_weights(self.__model.get_weights())\n",
        "\n",
        "    def save_model(self):\n",
        "        # save the model in case you want to load it later\n",
        "        self.__model.save(f'model-{self.__uuid}.keras')\n",
        "\n",
        "    def replay(self):\n",
        "        if len(self.__memory) <= self.__sample_batch_size:\n",
        "            # we wait until there is enough data in the memory before we start the learning proces\n",
        "            return\n",
        "        else:\n",
        "            # sample a random mini batch from the memory\n",
        "            mini_batch = random.sample(self.__memory, self.__sample_batch_size)\n",
        "\n",
        "            # build data structures for storing the data from the batch size\n",
        "            current_state = np.zeros((self.__sample_batch_size, self.__state_size))\n",
        "            next_state = np.zeros((self.__sample_batch_size, self.__state_size))\n",
        "            action = np.zeros(self.__sample_batch_size, dtype=int)\n",
        "            reward = np.zeros(self.__sample_batch_size)\n",
        "            done = np.zeros(self.__sample_batch_size,dtype=bool)\n",
        "\n",
        "            # fill the data structures with the data from our memory (tuples)\n",
        "            for i in range(self.__sample_batch_size):\n",
        "                current_state[i] = mini_batch[i][0]   # state\n",
        "                action[i] = mini_batch[i][1]          # action\n",
        "                reward[i] = mini_batch[i][2]          # reward\n",
        "                next_state[i] = mini_batch[i][3]      # next_state\n",
        "                done[i] = mini_batch[i][4]            # done\n",
        "\n",
        "            # we use double qlearning to stability reasons\n",
        "            # https://ojs.aaai.org/index.php/AAAI/article/view/10295\n",
        "\n",
        "            # use the first state to predict both q values\n",
        "            main_qvalues_state = self.__model.predict(current_state,verbose=0)\n",
        "\n",
        "            # use the next state to predict both q values\n",
        "            main_qvalues_new_state = self.__model.predict(next_state,verbose=0)\n",
        "\n",
        "            # use the next state to predict both q value, but this time we use the target model\n",
        "            target_qvalues_new_state = self.__target_model.predict(next_state,verbose=0)\n",
        "\n",
        "            # for each sample in the mini batch\n",
        "            for i in range(self.__sample_batch_size):\n",
        "                if done[i]:\n",
        "                    # no need to look further , there is no next step\n",
        "                    main_qvalues_state[i][action[i]] = reward[i]\n",
        "                else:\n",
        "                     # return the index of the best action (highest q-value for our new state)\n",
        "                    max_action_from_main_model = np.argmax(main_qvalues_new_state[i])\n",
        "\n",
        "                    main_qvalues_state[i][action[i]] = reward[i] + self.__gamma * target_qvalues_new_state[i][max_action_from_main_model]\n",
        "\n",
        "            # we ask the model to optimize itself using the first state and information that we gained through experience\n",
        "            self.__model.fit(current_state, main_qvalues_state, batch_size=self.__sample_batch_size, epochs=1, verbose=0,callbacks=[memory_clear_callback])\n",
        "\n",
        "            # increment the step counter and model synchronization logic\n",
        "            if self.__internal_step_counter % self.__update_model_steps == 0:\n",
        "                self._synch_models()\n",
        "\n",
        "            # update the counter\n",
        "            self.__internal_learn_counter +=1\n",
        "\n",
        "            # we use the decay factor to reduce our chance of exploration if we are not on the lower limit\n",
        "            if self.__exploration_rate > self.__exploration_min:\n",
        "                self.__exploration_rate *= self.__exploration_decay\n",
        "\n",
        "\n",
        "    def act(self, state: np.array) -> int:\n",
        "        if np.random.rand() < self.__exploration_rate:\n",
        "            # take a random action\n",
        "            random_action = self.__env.action_space.sample()\n",
        "            return random_action\n",
        "        else:\n",
        "            # use the model to make a prediction\n",
        "            actions = self.__model.predict(state, verbose=0)             # get a model prediction\n",
        "            best_action = np.argmax(actions[0])                          # get the output index with the highest value\n",
        "            return best_action\n",
        "\n",
        "    def _build_model(self):\n",
        "        model = Sequential()\n",
        "        model.add(Dense(self.__layer_size_1, input_dim=self.__state_size, activation='relu'))\n",
        "        model.add(Dense(self.__layer_size_2, activation='relu'))\n",
        "        if (self.__extra_intermediate_layers > 0):\n",
        "            for i in range(self.__extra_intermediate_layers + 1):\n",
        "                model.add(Dense(self.__layer_size_2, activation='relu'))\n",
        "        model.add(Dense(self.__action_size, activation='linear'))\n",
        "        model.compile(loss='mse', optimizer=tf.keras.optimizers.legacy.Adam(learning_rate=self.__learning_rate))\n",
        "        return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N3PP8R26WHup"
      },
      "outputs": [],
      "source": [
        "# train loop for the deep q agent\n",
        "def optimize_train(agent, env, observation_space_size, n_episodes):\n",
        "    # print('starting traing of agent ...')\n",
        "\n",
        "    for i in range(0, n_episodes):\n",
        "        # reset and reshape the state\n",
        "        state = np.reshape(env.reset()[0], [1, observation_space_size])\n",
        "\n",
        "        print(f'new train episode {i} on : {datetime.datetime.now()}')\n",
        "\n",
        "        # reset the internal state (learning counter)\n",
        "        agent.reset_episode()\n",
        "\n",
        "        # initialize done (fail) and truncated (success)\n",
        "        done = False\n",
        "        truncated = False\n",
        "\n",
        "         # execute the garbase collector\n",
        "        gc.collect()\n",
        "\n",
        "        # clear all the memory\n",
        "        keras.backend.clear_session()\n",
        "\n",
        "        # logging purpose counter\n",
        "        steps = 0\n",
        "\n",
        "        while not (done or truncated):\n",
        "\n",
        "            steps += 1\n",
        "\n",
        "            # take action based on the state\n",
        "            action = agent.act(state)\n",
        "\n",
        "            # observe the signal from the environment after the action\n",
        "            next_state, reward, done, truncated, info = env.step(action)\n",
        "\n",
        "            # enhance the signal and merge the truncated and done signals\n",
        "            reward = enhance_reward_signal(next_state, reward, done, truncated, steps)\n",
        "\n",
        "            if done or truncated:\n",
        "                print(f'ended training episode with done {done} - truncated {truncated} - reward {reward} - step count {steps}')\n",
        "\n",
        "            # reshape the state\n",
        "            next_state = np.reshape(next_state, [1, observation_space_size])\n",
        "\n",
        "            # add the observation to the memory\n",
        "            agent.add_to_memory(state, action, reward, next_state, done)\n",
        "\n",
        "            # replay observations from the memory and learn if needed\n",
        "            agent.replay()\n",
        "\n",
        "            # replace the current state\n",
        "            state = next_state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gfXpyMZLVK0O"
      },
      "outputs": [],
      "source": [
        "def objective(trial: optuna.trial.Trial) -> float:\n",
        "\n",
        "    # create an environment\n",
        "    env = gym.make('CartPole-v1',render_mode = \"rgb_array\")\n",
        "\n",
        "    # fetch properties from environment\n",
        "    observation_space_size = env.observation_space.shape[0]\n",
        "    action_space_size = env.action_space.n\n",
        "\n",
        "    print(f'observation space size : {observation_space_size}')\n",
        "    print(f'action space size      : {action_space_size}')\n",
        "\n",
        "    # get the hyper parameter config\n",
        "    args = sample_hyper_parameters(trial)\n",
        "\n",
        "    print(f'optuna run with params started with params : {trial.params}')\n",
        "\n",
        "    # create the deep agent\n",
        "    deep_agent = DeepQAgent(\n",
        "        env,\n",
        "        memory_size = args['memory_size'],\n",
        "        gamma = args['gamma'],\n",
        "        exploration_decay = args['exploration_decay'],\n",
        "        layer_size = args['layer_size'],\n",
        "        batch_size = args['batch_size'],\n",
        "        exploration_min = args['exploration_min'],\n",
        "        learning_rate = args['learning_rate'],\n",
        "        update_model_steps = args['update_model_steps'],\n",
        "        extra_intermediate_layers = args['extra_intermediate_layers']\n",
        "    )\n",
        "\n",
        "    print('train loop started')\n",
        "\n",
        "    # reset all\n",
        "    env.reset()\n",
        "\n",
        "    # train loop\n",
        "    optimize_train(deep_agent,env, observation_space_size,n_episodes=args['episodes'])\n",
        "\n",
        "    # disable exploitation and only rely on the model for decisions\n",
        "    deep_agent.disable_exploration()\n",
        "\n",
        "    print('evaluation started')\n",
        "    rewards, steps = evaluate(deep_agent, env, 50, observation_space_size)\n",
        "\n",
        "    # evaluation results\n",
        "    median_steps = np.median(steps)\n",
        "    mean_steps = np.mean(steps)\n",
        "    std_steps = np.std(steps)\n",
        "\n",
        "    median_reward = np.median(rewards)\n",
        "    mean_reward = np.mean(rewards)\n",
        "    std_reward= np.std(rewards)\n",
        "\n",
        "\n",
        "    print(f'median reward = {median_reward}')\n",
        "    print(f'std reward    = {std_reward}')\n",
        "    print(f'mean reward   = {mean_reward}')\n",
        "\n",
        "    print(f'median steps = {median_steps}')\n",
        "    print(f'std steps    = {std_steps}')\n",
        "    print(f'mean steps   = {mean_steps}')\n",
        "\n",
        "    # record the agent\n",
        "    record_trained_agent(deep_agent,env)\n",
        "\n",
        "    # clear all the memory\n",
        "    keras.backend.clear_session()\n",
        "\n",
        "    # execute the garbase collector\n",
        "    gc.collect()\n",
        "\n",
        "    return median_reward"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s8doAIy3VTKM"
      },
      "outputs": [],
      "source": [
        "# create an optimized model\n",
        "study_name = \"rl-study\"\n",
        "storage_name = \"sqlite:///{}.db\".format(study_name)\n",
        "study = optuna.create_study(study_name=study_name, storage=storage_name,direction='maximize',load_if_exists=True)\n",
        "\n",
        "study.optimize(objective, n_trials=5, gc_after_trial=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IooChA6qgXih"
      },
      "outputs": [],
      "source": [
        "# print param importances\n",
        "print(get_param_importances(study))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-pysxFBHgSNJ"
      },
      "outputs": [],
      "source": [
        "df = study.trials_dataframe()\n",
        "df.head(20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fKjJ_3D6iITv"
      },
      "outputs": [],
      "source": [
        "study.best_params"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
