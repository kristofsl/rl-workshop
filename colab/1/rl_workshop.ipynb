{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "btOKapbhZlFX"
      },
      "outputs": [],
      "source": [
        "!pip install optuna==3.2.0\n",
        "!pip install gymnasium==0.28.1\n",
        "!pip install renderlab",
        "!pip install keras==2.13.1",
        "!pip install tensorflow==2.13.0rc2",
        "!pip install tensorflow-estimator==2.13.0",
        "!pip install tensorflow-macos==2.13.0rc2",
        "!pip install tensorboard==2.13.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FhHxtnSXZpaO"
      },
      "outputs": [],
      "source": [
        "from typing import Tuple, List, Callable, Union, Optional, Dict\n",
        "import numpy as np\n",
        "import os\n",
        "import pathlib\n",
        "import imageio\n",
        "from pathlib import Path\n",
        "import pandas as pdbb\n",
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from matplotlib import pyplot as plt\n",
        "from pathlib import Path\n",
        "from gym.wrappers import RecordVideo\n",
        "import datetime\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "from collections import deque\n",
        "import time\n",
        "import random\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.optimizers import Adam\n",
        "import os\n",
        "import uuid\n",
        "import renderlab as rl\n",
        "import gc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YSYGVxOayXvx"
      },
      "outputs": [],
      "source": [
        "def evaluate(\n",
        "    agent,\n",
        "    env,\n",
        "    n_episodes: int,\n",
        "    observation_space_size: int\n",
        ") -> Tuple[List, List]:\n",
        "\n",
        "    # keep track of the reward and steps per episode\n",
        "    reward_per_episode = []\n",
        "    steps_per_episode = []\n",
        "\n",
        "    for i in range(0, n_episodes):\n",
        "\n",
        "        # reset and initial state from environment needs reshape to np array (tuple instead of array with dimensions 1,4)\n",
        "        state = np.reshape(env.reset()[0], [1, observation_space_size])\n",
        "\n",
        "        # initialize the cumulative steps and the rewards for this episode\n",
        "        rewards = 0\n",
        "        steps = 0\n",
        "\n",
        "        # done = environment ended badly / truncated = environment ended after 500 episodes\n",
        "        done = False\n",
        "        truncated = False\n",
        "\n",
        "        while not (done or truncated):\n",
        "\n",
        "            # determine action based on the state\n",
        "            action = agent.act(state)\n",
        "\n",
        "            # take the action and observe the reward, new state, etc\n",
        "            new_state, reward, done, truncated, info = env.step(action)\n",
        "\n",
        "            # enchange the reward signal\n",
        "            reward = enhance_reward_signal(new_state, reward, done, truncated, steps)\n",
        "\n",
        "            # add the data from this step to the episode data\n",
        "            rewards += reward\n",
        "            steps += 1\n",
        "\n",
        "            # reshape the new state\n",
        "            state = np.reshape(new_state, [1, observation_space_size])\n",
        "\n",
        "        reward_per_episode.append(rewards)\n",
        "        steps_per_episode.append(steps)\n",
        "\n",
        "    return reward_per_episode, steps_per_episode"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YQPyQG-5yncg"
      },
      "outputs": [],
      "source": [
        "# random agent\n",
        "class RandomAgent:\n",
        "    def __init__(self, env):\n",
        "        self.env = env\n",
        "\n",
        "    def act(self, state: np.array) -> int:\n",
        "        return self.env.action_space.sample()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mTzg3HsVzoGF"
      },
      "outputs": [],
      "source": [
        "def enhance_reward_signal(next_state, reward, done, truncated, steps):\n",
        "    if done:\n",
        "        # this is bad and you did not manage to keep the pole in the air\n",
        "        reward = -100\n",
        "    elif truncated:\n",
        "        # you probably reached the time limit\n",
        "        reward = 100\n",
        "    elif next_state[0] > 0.2:\n",
        "        # the pole is getting to the border of the playground\n",
        "        reward = -5\n",
        "    elif next_state[0] < -0.2:\n",
        "        # the pole is getting to the border of the playground\n",
        "        reward = -5\n",
        "\n",
        "    return reward"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W4TRoRYKytET"
      },
      "outputs": [],
      "source": [
        "def record_random_agent():\n",
        "    # trying the random agent first\n",
        "    env = gym.make('CartPole-v1',render_mode = \"rgb_array\")\n",
        "    env = rl.RenderFrame(env, \"./output\")\n",
        "\n",
        "    observation_space_size = env.observation_space.shape[0]\n",
        "\n",
        "    # create a random agent\n",
        "    random_agent = RandomAgent(env)\n",
        "\n",
        "    # evaluate the random agent\n",
        "    rewards, steps = evaluate(random_agent, env, 5, observation_space_size)\n",
        "\n",
        "    env.play()\n",
        "\n",
        "    # evaluation results\n",
        "    median_steps = np.median(steps)\n",
        "    mean_steps = np.mean(steps)\n",
        "    std_steps = np.std(steps)\n",
        "\n",
        "    print(f'median steps = {median_steps}')\n",
        "    print(f'std steps    = {std_steps}')\n",
        "    print(f'mean steps   = {mean_steps}')\n",
        "\n",
        "    return rewards, steps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l0TGQmNuplDJ"
      },
      "outputs": [],
      "source": [
        "def record_trained_agent(agent, env):\n",
        "    env = rl.RenderFrame(env, \"./output\")\n",
        "\n",
        "    observation_space_size = env.observation_space.shape[0]\n",
        "\n",
        "    # evaluate the random agent\n",
        "    rewards, steps = evaluate(agent, env, 1, observation_space_size)\n",
        "\n",
        "    env.play()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "huM3ocq5yZME"
      },
      "outputs": [],
      "source": [
        "# record a random agent\n",
        "rewards, steps = record_random_agent()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M6_conWu0S0i"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "fig, ax = plt.subplots(figsize = (10, 4))\n",
        "ax.set_title(\"Steps\")\n",
        "pd.Series(steps).plot(kind='hist', bins=100)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GKZTzlQdbfh4"
      },
      "outputs": [],
      "source": [
        "import gc\n",
        "from keras.callbacks import Callback\n",
        "\n",
        "class MemoryClear(Callback):\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fIWyHHHLU7is"
      },
      "outputs": [],
      "source": [
        "memory_clear_callback = MemoryClear()\n",
        "\n",
        "class DeepQAgent:\n",
        "\n",
        "    def __init__(self, env, memory_size, gamma, exploration_decay, layer_size, batch_size, exploration_min, learning_rate, extra_intermediate_layers = 0, exploration_rate = 1.0):\n",
        "        self.__env = env\n",
        "        self.__state_size = 4                                           # we have an 4 numbers that represent our size and this will be the input for our neural net\n",
        "        self.__action_size = 2                                          # we have 2 possible actions (push the card to the left or to the right)\n",
        "        self.__memory = deque(maxlen=memory_size)                       # memory for storing our experiences\n",
        "        self.__layer_size_1 = layer_size                                # nn layer 1 width\n",
        "        self.__layer_size_2 = layer_size                                # nn layer 2 width\n",
        "        self.__extra_intermediate_layers = extra_intermediate_layers    # extra intermediate layers?\n",
        "        self.__learning_rate = learning_rate                            # learning rate\n",
        "        self.__model = self._build_model()                              # our model\n",
        "        self.__gamma = gamma                                            # discount rate\n",
        "        self.__exploration_rate = exploration_rate                      # exploration rate\n",
        "        self.__exploration_min = exploration_min                        # min value for exploration\n",
        "        self.__exploration_decay = exploration_decay                    # decay in the exploration rate (moving from exploration to exploitation)\n",
        "        self.__step_counter = 0                                         # keep track of the steps and update the target model after target_model_delay steps\n",
        "        self.__sample_batch_size = batch_size                           # how much samples to we take for the learning step\n",
        "        self.__uuid = str(uuid.uuid4())                                 # unique uid for the model\n",
        "\n",
        "\n",
        "    def add_to_memory(self, state, action, reward, next_state, done):\n",
        "        # add an experience to the memory\n",
        "        self.__memory.append((state, action, reward, next_state, done))\n",
        "\n",
        "    def disable_exploration(self):\n",
        "        # disable exploration and the agent will always use the model to determine the optimal action\n",
        "        self.__exploration_rate = 0\n",
        "        self.__exploration_min = 0\n",
        "\n",
        "    def get_agent_uuid(self):\n",
        "        return self.__uuid\n",
        "\n",
        "    def save_model(self):\n",
        "        self.__model.save(f'model-{self.__uuid}.keras')\n",
        "\n",
        "    def replay(self):\n",
        "\n",
        "        if len(self.__memory) <= self.__sample_batch_size:\n",
        "            # there is not enough data in the memory to take a batch of samples to learn from\n",
        "            return\n",
        "        else:\n",
        "            # take a random batch of samples from the memory\n",
        "            mini_batch = random.sample(self.__memory, self.__sample_batch_size)\n",
        "\n",
        "            # create data structures for setting the batch data\n",
        "            # 2 dimensions : each sample has a state\n",
        "            current_state = np.zeros((self.__sample_batch_size, self.__state_size))\n",
        "            # 2 dimensions : each sample has a next state\n",
        "            next_state = np.zeros((self.__sample_batch_size, self.__state_size))\n",
        "            # 2 dimensions : each sample has a target q value\n",
        "            target_q_values = np.zeros((self.__sample_batch_size, self.__state_size))\n",
        "\n",
        "            # 1 dimension : each sample has 1 action\n",
        "            action = np.zeros(self.__sample_batch_size, dtype=int)\n",
        "            # 1 dimension : each sample has 1 reward\n",
        "            reward = np.zeros(self.__sample_batch_size)\n",
        "            # 1 dimension : each sample has 1 boolean that indicates terminal state\n",
        "            done = np.zeros(self.__sample_batch_size,dtype=bool)\n",
        "\n",
        "            # fill the data structure with data from the memory\n",
        "            for i in range(self.__sample_batch_size):\n",
        "                current_state[i] = mini_batch[i][0]   # state\n",
        "                action[i] = mini_batch[i][1]          # action\n",
        "                reward[i] = mini_batch[i][2]          # reward\n",
        "                next_state[i] = mini_batch[i][3]      # next_state\n",
        "                done[i] = mini_batch[i][4]            # done\n",
        "\n",
        "            # what is our current belief of the world?\n",
        "            # what would we do if we didn't take the action?\n",
        "            # (q-values for our current state before taking the action)\n",
        "            target = self.__model.predict(current_state,verbose=0)\n",
        "\n",
        "            # what are the q values for our next state (after taking the action)\n",
        "            Qvalue_ns = self.__model.predict(next_state,verbose=0)\n",
        "\n",
        "            # let's update the target with the new information (that was gained after taking the action)\n",
        "            for i in range(self.__sample_batch_size):\n",
        "                if done[i]:\n",
        "                    # if it's a terminal state, then it's just the reward\n",
        "                    target[i][action[i]] = reward[i]\n",
        "                else:\n",
        "                    # use the bellman equation principle to update the target information for the q value that you actually took\n",
        "                    target[i][action[i]] = reward[i] + self.__gamma * (np.amax(Qvalue_ns[i]))\n",
        "\n",
        "            # so now your target network output is updated and we tell the network to adjust it's weights and adapt to that new target (new q-value for the action that you took)\n",
        "            self.__model.fit(current_state, target, batch_size=self.__sample_batch_size, epochs=1, verbose=0, callbacks=[memory_clear_callback])\n",
        "\n",
        "            # decay the exploration rate\n",
        "            if self.__exploration_rate > self.__exploration_min:\n",
        "                self.__exploration_rate *= self.__exploration_decay\n",
        "\n",
        "\n",
        "    def act(self, state: np.array) -> int:\n",
        "        # the exploration rate will decrease over time\n",
        "        if np.random.rand() < self.__exploration_rate:\n",
        "            random_action = self.__env.action_space.sample()\n",
        "            return random_action\n",
        "        else:\n",
        "            # get a model prediction\n",
        "            actions = self.__model.predict(state, verbose=0)\n",
        "            # get the output index with the highest value\n",
        "            best_action = np.argmax(actions[0])\n",
        "            return best_action\n",
        "\n",
        "    def _build_model(self):\n",
        "        model = Sequential()\n",
        "        model.add(Dense(self.__layer_size_1, input_dim=self.__state_size, activation='relu'))\n",
        "        model.add(Dense(self.__layer_size_2, activation='relu'))\n",
        "        if (self.__extra_intermediate_layers > 0):\n",
        "            for i in range(self.__extra_intermediate_layers + 1):\n",
        "                model.add(Dense(self.__layer_size_2, activation='relu'))\n",
        "        model.add(Dense(self.__action_size, activation='linear'))\n",
        "        model.compile(loss='mse', optimizer=tf.keras.optimizers.legacy.Adam(learning_rate=self.__learning_rate))\n",
        "        return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N3PP8R26WHup"
      },
      "outputs": [],
      "source": [
        "# train loop for the deep q agent\n",
        "def train(agent, env, observation_space_size, n_episodes):\n",
        "    # print('starting traing of agent ...')\n",
        "\n",
        "    for i in range(0, n_episodes):\n",
        "        # reset and reshape the state\n",
        "        state = np.reshape(env.reset()[0], [1, observation_space_size])\n",
        "\n",
        "        print(f'new train episode {i} on : {datetime.datetime.now()}')\n",
        "\n",
        "        # initialize done (fail) and truncated (success)\n",
        "        done = False\n",
        "        truncated = False\n",
        "\n",
        "        steps = 0\n",
        "\n",
        "        # execute the garbase collector\n",
        "        gc.collect()\n",
        "\n",
        "        # clear all the memory\n",
        "        keras.backend.clear_session()\n",
        "\n",
        "        while not (done or truncated):\n",
        "\n",
        "            steps += 1\n",
        "\n",
        "            # take action based on the state\n",
        "            action = agent.act(state)\n",
        "\n",
        "            # observe the signal from the environment after the action\n",
        "            next_state, reward, done, truncated, info = env.step(action)\n",
        "\n",
        "            # enhance the signal and merge the truncated and done signals\n",
        "            reward = enhance_reward_signal(next_state, reward, done, truncated, steps)\n",
        "\n",
        "            if done:\n",
        "                print(f'ended training episode with mistake and reward {reward} and step count {steps}')\n",
        "            elif truncated:\n",
        "                print(f'ended training episode with success and reward {reward} and step count {steps}')\n",
        "\n",
        "            next_state = np.reshape(next_state, [1, observation_space_size])\n",
        "\n",
        "            # add the observation to the memory\n",
        "            agent.add_to_memory(state, action, reward, next_state, done)\n",
        "\n",
        "            # replay observations from the memory and learn if needed\n",
        "            agent.replay()\n",
        "\n",
        "            # replace the current state\n",
        "            state = next_state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gfXpyMZLVK0O"
      },
      "outputs": [],
      "source": [
        "# create an environment\n",
        "env = gym.make('CartPole-v1',render_mode = \"rgb_array\")\n",
        "\n",
        "# fetch properties from environment\n",
        "observation_space_size = env.observation_space.shape[0]\n",
        "action_space_size = env.action_space.n\n",
        "\n",
        "print(f'observation space size : {observation_space_size}')\n",
        "print(f'action space size      : {action_space_size}')\n",
        "\n",
        "# TODO FILL IN\n",
        "p_memory_size = 500                          # value from 500 - 5000\n",
        "p_gamma = 0.9                                # value from 0.9 - 0.99\n",
        "p_batch_size = 16                           # value from either [16,32,64,128]\n",
        "p_exploration_decay = 0.9                     # value from 0.9 - 0.99\n",
        "p_layer_size = 16                           # value from either [16,32,64,128,256]\n",
        "p_exploration_min = 0.1                      # value from 0.0001 - 0.2\n",
        "p_episodes = 150                             # value from 150 - 500\n",
        "p_learning_rate = 0.001                         # value from 0.0001 - 0.001\n",
        "p_extra_intermediate_layers = 0            # value from 0 - 1\n",
        "p_episodes = 50                             # value from 100 - 1000\n",
        "\n",
        "deep_agent = DeepQAgent(\n",
        "    env,\n",
        "    memory_size = p_memory_size,\n",
        "    gamma = p_gamma,\n",
        "    exploration_decay = p_exploration_decay,\n",
        "    layer_size = p_layer_size,\n",
        "    batch_size = p_batch_size,\n",
        "    exploration_min = p_exploration_min,\n",
        "    learning_rate = p_learning_rate,\n",
        "    extra_intermediate_layers = p_extra_intermediate_layers,\n",
        ")\n",
        "\n",
        "print('train loop started')\n",
        "\n",
        "# reset all\n",
        "env.reset()\n",
        "\n",
        "# train loop\n",
        "train(deep_agent,env, observation_space_size,n_episodes=p_episodes)\n",
        "\n",
        "# disable exploitation and only rely on the model for decisions\n",
        "deep_agent.disable_exploration()\n",
        "\n",
        "print('evaluation started')\n",
        "rewards, steps = evaluate(deep_agent, env, 50, observation_space_size)\n",
        "\n",
        "# evaluation results\n",
        "median_steps = np.median(steps)\n",
        "mean_steps = np.mean(steps)\n",
        "std_steps = np.std(steps)\n",
        "\n",
        "median_reward = np.median(rewards)\n",
        "mean_reward = np.mean(rewards)\n",
        "std_reward= np.std(rewards)\n",
        "\n",
        "\n",
        "print(f'median reward = {median_reward}')\n",
        "print(f'std reward    = {std_reward}')\n",
        "print(f'mean reward   = {mean_reward}')\n",
        "\n",
        "print(f'median steps = {median_steps}')\n",
        "print(f'std steps    = {std_steps}')\n",
        "print(f'mean steps   = {mean_steps}')\n",
        "\n",
        "# record the agent\n",
        "record_trained_agent(deep_agent,env)\n",
        "\n",
        "# clear all the memory\n",
        "keras.backend.clear_session()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
